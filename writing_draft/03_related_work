In this section, we will review research that is relevant to this study. Although there is
limited research directly focused on using Large Language Models (LLMs) for automated
ontology data model mapping, several studies and techniques are related to this
task.

3.1 Traditional Approaches to heterogenous data model mapping 
previously is heavily rely on expert knowledge. 
3.1.1 Structured data like HTML
The majority of works focus on structuringHTML [12, 15, 25], assuming the attributes and values are at specific
positions in the HTML-DOM [22, 45, 46, 71], and for file-format\cite{xtract}. For unstructured text, current approaches use linguistic tools (e.g., dependency parsers) to introduce structure [15, 25, 31, 51] and then apply heuristic rules over the resulting structure to extract information.

3.1.2 Unstructured data
some assumed human-in-a-loop, some assumed access to annotated training documents from the domain.





3.2 heterogenous data model mapping using LLMs
An LLM is a deep learning model that is pretrained on broad data and can be adapted to diverse tasks, from machine translation to data wrangling. with the development of LLM, it can find semantic relationships between words, therefore provide better performance on ontology matching. 



3.2.1 direct extraction
use prompt to directly identify and extract values, but the cost is expensive. cost-wise: As of March 2023,
applying OpenAI’s models to the 55 million Wikipedia articles would cost over $110k (gpt-3.5, $0.002/1k tokens) and $1.1M (text-davinci-003, $0.02/1k tokens) dollars [1, 52].
'''List all attributes about the player mentioned in this document. '''
we discuss how (1) manage long documents that cannot fit in the LLM's context window, (2) process the LLM's texual outputs, (3) prioritize the most useful attributes according to principles described in prior worl.
\cite{symphony} presents a system for querying heterogeneous data lakes with in-context learning.
这篇文章还没有看



3.2.2 tuning prompts to a domain 
\cite{data_wrangling} this paper introduces fine-tuning propmts to a domain.


3.2.2 use LLM to generate code 
(1)schema generation: we process a small sample of documents with the LLM. 
(2)function generation: use LLM to generate python code.
while it address the cost issues of Direct extraction, the quality of the result is 14% worse than using direct extraction.

To improve the quality problem, they applied weak supervision to prompting for the first time, Weak supervision (WS) is a statistical framework for modeling and combining noisy sources with varied coverages without any labeled data. The current issue when attempting to apply existing WS tools is (1) WS theoretically assumes all noisy sources are better than random performance (50% accuracy), yet 40% of our generated functions are below 25%. (2) WS attempts to deploy functions that achieve high quality on narrow slices of data (high precision), and allow the function to abstain on data external to the slice (low recall). While humans can express when functions should abstain, the machine-generated functions do not contain this logic.[which is where the hallucination happens, when it has a bad recall, it doesn't know when to ditch the answer. ]

They bring up this solution for unstable output generated by LLM, they curates multiple function generation prompts to produce a diverse set of function candidates instead of only generating one candidate function. Here we can compare zero-shot and few-shots template.

The reason why we cannot use weak supervision is that there's three assumptions that's not applied here: (1) Functions will abstain on examples where they do not apply, while the reality is when a function return void, it can be two reasons: the attribute does not exist in the document or the attribute exists but the function failed to extract. The traditional WS setup is chanllenging to distinguish the difference. (2) Functions are correlated with the gold label y at better than random performance. (3) Weak supercision is typically applied to tasks with well defined classes in a classification setting.


3.3 a prototype system powered by language models


===========================================================================


@misc{data_lake,
      title={Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes}, 
      author={Simran Arora and Brandon Yang and Sabri Eyuboglu and Avanika Narayan and Andrew Hojel and Immanuel Trummer and Christopher Ré},
      year={2025},
      eprint={2304.09433},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.09433}, 
}



@article{machine_translation,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{data_wrangling,
  title={Can foundation models wrangle your data?},
  author={Narayan, Avanika and Chami, Ines and Orr, Laurel and Arora, Simran and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.09911},
  year={2022}
}

@inproceedings{xtract,
  title={XTRACT: A system for extracting document type descriptors from XML documents},
  author={Garofalakis, Minos and Gionis, Aristides and Rastogi, Rajeev and Seshadri, Sridhar and Shim, Kyuseok},
  booktitle={Proceedings of the 2000 ACM SIGMOD international conference on Management of data},
  pages={165--176},
  year={2000}
}

@inproceedings{symphony,
  title={Symphony: Towards Natural Language Query Answering over Multi-modal Data Lakes.},
  author={Chen, Zui and Gu, Zihui and Cao, Lei and Fan, Ju and Madden, Samuel and Tang, Nan},
  booktitle={CIDR},
  pages={1--7},
  year={2023}
}

